# -*- coding: utf-8 -*-
"""
Created on Tue Sep  6 13:05:16 2016

@author: robsalz

A script that takes a foo.csv file with one of the column fields containing 
natural language descriptions and returns a foo_methodx.npy which is an array of 
vector representations for foo descriptions generated by methodx 
"""

import re
import math
import random
import collections
import sys

import tensorflow as tf
import numpy as np
import pandas as pd


from nltk import RegexpTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer


import os
import sys
import threading
import time

from six.moves import xrange  # pylint: disable=redefined-builtin

import numpy as np
import tensorflow as tf

from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.models import word2vec

flags = tf.app.flags
flags.DEFINE_string("file", 'mill_logs.csv', "Input csv file")
flags.DEFINE_string("field", 'description', "The field in the csv file that has natural language logs")
flags.DEFINE_integer("tk", 1,'tokenization method')
flags.DEFINE_string("method", 'tfidf', "Vectorization Method")

#Flags for the TFIDF Vectorizer
flags.DEFINE_float('max_df',0.95,'''(0 - 1) Ignore terms that have a document frequency higher than threshold ''')
flags.DEFINE_float('min_df',0.001,'''(0 - 1) Ignore terms that have a document frequency higher than threshold ''')
flags.DEFINE_integer('max_features',2000, 'Upper limit on the number of features in tfidf matrix')
flags.DEFINE_integer('n_gram',1, 'Upper limit on the number of n grams')

#Flags for word to vec vectorizer
flags.DEFINE_string("emb_size", 100, 'The word embedding dimension')
flags.DEFINE_string("window", 5, 'The size of the context window')

FLAGS = flags.FLAGS
class Vectorize(object):
    '''Base class for all the vectorization classes
    Attributes
    file: (string) Path to the log file.csv
    field: (string) Field containg natural language data in file.csv
    tk: (int) Option to set the tokenization method 
    data: (pd.DataFrame) Data frame of the CSV file
    text: (pd.Series) Series of document strings
    tk_text: List of tokenized documents
    '''
    def __init__(self):
        self.file = FLAGS.file
        self.field = FLAGS.field
        self.tk = FLAGS.tk
        self._read_file()
        self.vocab = {}
        self.inv_vocab = []
        
        self._tokenize()
        self.vectorize()
#        self._build_vocab()
#        self._word2id()
        
    def _read_file(self):
        'Function to read file.csv to pd.DataFrame and pd.Series of text'
        self.data = pd.read_csv(self.file)
        self.text = self.data[self.field]
        
    def _tokenize(self):
        '''Converts a list of documents into a list of lists of strings based on 
        the method used to tokenize the documentation'''
        self.tk_text = []
        for document in self.text:
            if self.tk == 1:
                tokenizer = RegexpTokenizer(r"""([A-Za-z]+[/|-|\][A-Za-z]+|[A-Za-z]+)""")
                tokens = [word.lower() for word in tokenizer.tokenize(document)]
                self.tk_text.append(tokens)
        
#    def _build_vocab(self):
#        '''Creates the vocabulary and inv_vocabularies from the text'''
#        self.tk_text
#        
#    def _word2id(self):
#        '''
#        Uses the vocubulary to convert tokenized documents into lists of token ids
#        '''
    
        
    
#    def tokenize():

#    def build():
#        
#    def save_path():


class TFIDF(Vectorize):
    '''
    Parameters:
    max_df
    min_df
    ngram_range
    '''     
    def vectorize(self):
        '''Creates a vector matrix corresponding to the documents in the csv file
        with specified field and saves the numpy file with the correct naming convention'''
        self.tfidf_vectorizer = TfidfVectorizer(max_features=FLAGS.max_features,
                                 min_df = FLAGS.min_df, max_df = FLAGS.max_df,
                                 use_idf=True, ngram_range = (1,FLAGS.n_gram))
        self.doc_vectors = self.tfidf_vectorizer.fit_transform(self.text).todense()       
    def save_vector(self):
        '''Saves the vectorized documnents as a numpy array with standard naming convention''' 
        np.save(save_dir, self.doc_vectors)
        self.save_dir = self.file.replace('.csv','')+'_'+FLAGS.method+'_%s_%s_%d' %(FLAGS.min_df,FLAGS.max_df,FLAGS.n_gram)

        
class Word2Vec(Vectorize):
    '''
    Nothing of importance just yet
    '''
    model = word2vec.Word2Vec(self.tk_text, workers=4, \
            size = FLAGS.emb_size, min_count = 1, \
            window = FLAGS.window, sample = 0.001)

# If you don't plan to train the model any further, calling 
# init_sims will make the model much more memory-efficient.
#model.init_sims(replace=True)

# It can be helpful to create a meaningful model name and 
#save the model for later use. You can load it later using Word2Vec.load()
model_name = "100features_1minwords_5context"
model.save(model_name)
    def vectorize(self):
        '''Creates a vector matrix corresponding to the documents in the csv file
        with specified field and saves the numpy file with the correct naming convention'''
           
    def save_vector(self):
        '''Saves the vectorized documnents as a numpy array with standard naming convention''' 
        np.save(save_dir, self.doc_vectors)
        self.save_dir = self.file.replace('.csv','')+'_'+FLAGS.method+'_%s_%s_%d' %(FLAGS.min_df,FLAGS.max_df,FLAGS.n_gram)


class Doc2Vec(Vectorize):
    '''
    Nothing of importance just yet
    '''

#
#        
#class W2V_Options(object):
#    """Options used by our word2vec model."""
#    def __init__(self):
#        # Model options.
#
#
#        # Training options.
#        # The training text file.
#        self.train_data = FLAGS.train_data
# 
#        # Where to write out summaries.
#        self.save_path = FLAGS.save_path
#
#
#
#FLAGS = tf.app.flags()
#
#tk = FLAG.DEFINE_integer('tk', 1, 'Specifies the method for tokenizing the data')
#                   
#def tokenize(text, tk = 1):
#    '''Tokenize using a regular expression We use consecutive letters 
#    (exclude punction and numbers)
#    
#    Parameters:
#    text:(string) representing a document
#    tk: (int) The tokenization method to be used
#        1. 
#        2.
#        3.
#        4.
#    
#    Returns:
#    tokens: List of strings tokenized according to option tk
#    '''
#    #TODO Research, implement and document proper tokenization
#    if tk == 1:
#        tokenizer = RegexpTokenizer(r"""([A-Za-z]+[/|-|\][A-Za-z]+|[A-Za-z]+)""")
#        tokens = [word.lower() for word in tokenizer.tokenize(text)]
#    elif tk == 2:
#        tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]
#        filtered_tokens = []
#    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)
#        for token in tokens:
#            if re.search('[a-zA-Z]', token):
#                filtered_tokens.append(token)
#        tokens = filtered_tokens   
#    return filtered_tokens      
#    
#
#class tfidf(object):
#    ''' A function that takes a list of documents and returns a numpy array of
#    document vectors and writes it to file'''
#    def __init__(self, options, session):
#        self._options = options
#        
#    def vectorize:
#        
#    def save:
#    
#    def load:
#        
#def
#    
#
#def tk3(documents):
#    
#class options(object) :
#    
#class tokenize(object):
#    
#class tfidf(object):
#    
#    
#class word2vec(object):
#    
#class doc2vec(object):
#    
#    
